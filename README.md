# deep voice2 学习

## 模型结构

deep voice2有两种结构,分别是单speaker和多speaker的模型

### 单speaker deep voice 2
**整体结构**

单speaker deep voice 2 保留了deep voice 1的主体结构,其推理部分结构如下:

![](https://github.com/sysu16340234/deep_voice2/blob/master/img/fig1.png)

**推理过程**大致如下:文本通过发音字典转换为音素,然后将音素输入音素持续时间模块进行持续时间预测,得到持续时间后和音素一起进行上采样,得到采样结果输入频率预测模块得到预测基频,与deep voice1不同的是,deep voice 2的持续时间和频率是分开预测而不是进行联合预测,最后将预测的基频和经过上采样的音素输入语音模块生成语音音频;

**训练过程**的图例如下:

![](https://github.com/sysu16340234/deep_voice2/blob/master/img/trainng.png)

可以看出训练过程大体上与deep voice 1很相似,但是加入了说话者的特征作为训练样本,文本经过发音字典或者字素到音素模型传唤为音素,语音通过频率提取得到基频,音素和音频在经过分段模块后得到分段后的语音,频率模块用音频,音素,频率特征和说话者特征来训练,持续时间模型用音素,分段语音和说话者特征来训练;语音模型用音频,音素,分段语音和说话者特征来训练;

**模型细节**

**1.分段模型**

deep voice 2的分段模型使用的依然是具有CTC loss的卷积-循环结构,但是在卷积层中增加了批量标准化和残差连接;

deep voice 1中的每层输出如下:

![](https://github.com/sysu16340234/deep_voice2/blob/master/img/1.png)

其中h(l)是第l层的输出,W(l)是卷积滤波器,b(l)是偏置向量

而deep voice 2的每层输出如下:

![](https://github.com/sysu16340234/deep_voice2/blob/master/img/2.png)

其中BN为批量标准化;

为了防止在解码静音时发生边界错误,在解码静音时采用启发式算法调整边界位置;

**2.持续时间预测模型**

与deep voice 1不同的是,deep voice 2并不连续预测持续时间,而是把持续时间分成多个经过对数变换的桶,将输入音素分配到对应的桶中,相当于把预测问题转化为序列标签问题;序列建模采用(Lample et al., 2016)的CRF模型;在推理过程中,使用前向-后向Viterbi算法来解码来自CRF的序列;

**3.频率模型**

预测的音素持续时间和音素经过上采样后作为频率模型的输入;

频率模型的层次如下:

1.输入由一个双向的GRU层来产生隐状态;

2.隐状态经过仿射投影和sigmoid层生成输入有声的概率分布;

3.隐状态被用来预测两个基频:第一个基频fGRU由一个单层双向GRU和一个仿射投影来预测;第二个基频fconv由若干个具有不同卷积宽度的单通道卷积相加得到;

4.隐状态经过仿射投影和sigmoid层生成混合比率ω;

5.用第4步的ω作为权重对两个预测基频加权求和得到标准化基频f:

![](https://github.com/sysu16340234/deep_voice2/blob/master/img/3.png)

6.标准化基频f通过如下方法转换为真实基频F0:

![](https://github.com/sysu16340234/deep_voice2/blob/master/img/4.png)

其中μF和σF分别是说话者样本基频的均值和标准差;

**4.音频合成模型**

音频合成模型采用和deep voice1相似的具有两层双向QRNN调整网络的WaveNet结构,但是删除了门控tanh和残差连接之间的1x1卷积,并且对WaveNet中的每一层采用相同的偏置值;

## 多speaker deep voice 2

**整体结构**

为了使模型能够生成多speaker的音频,为每个说话者增加一个低维嵌入向量,用这个嵌入来产生RNN的初始状态,非线性偏置和门控乘法算子;这些嵌入向量用一个[-0.1,0.1]的均匀分布来初始化,通过反向传播进行联合训练;

文中提到了四种提高模型效果的方法:

**使用点特定的说话者嵌入**:对于模型中每个使用到说话者嵌入向量的地方,通过仿射变换和非线性变换将嵌入向量变换到合适的维度和形式;

**循环初始化**:使用特定的说话者嵌入来初始化循环层;

**输入增加**: 在循环的每个时间步骤中,使用说话者嵌入向量与输入的连接来作为新的输入;

**特征门控**: 将每层激活函数的输出与嵌入向量点乘来生成适应性的门控;

**模型细节**

![](https://github.com/sysu16340234/deep_voice2/blob/master/img/deep_voice_2.png)

**1.分割模型**

与单speaker相比,采用新的方法计算隐状态h(l):

![](https://github.com/sysu16340234/deep_voice2/blob/master/img/5.png)

其中gs是说话者的嵌入向量,这是所有卷积层共享的参数,同样的,所有的RNN层也会共享另外一个嵌入参数;

**2.持续时间模型**

